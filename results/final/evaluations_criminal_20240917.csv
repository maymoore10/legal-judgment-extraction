filename	precision	recall	f1	accuracy	bleu	rouge	tlnls
summaries_2last_mbert-base-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.16351010101010102, recall=0.6727272727272727, fmeasure=0.26307770441848655), 'rouge2': Score(precision=0.08338597599494631, recall=0.34375, fmeasure=0.13421453990849008), 'rougeL': Score(precision=0.10164141414141414, recall=0.41818181818181815, fmeasure=0.16353478923311324)}	0.16146738364326252
summaries_1last_google-mbert-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.3426470588235294, recall=0.6051948051948052, fmeasure=0.43755868544600934), 'rouge2': Score(precision=0.10751104565537556, recall=0.19010416666666666, fmeasure=0.13734713076199434), 'rougeL': Score(precision=0.15441176470588236, recall=0.2727272727272727, fmeasure=0.1971830985915493)}	0.27391839170040716
information_retrieval_mbert-ft.csv	0.011135857461024499	0.011135857461024499	0.011135857461024499	0.011135857461024499	0.0007396114523916231	{'rouge1': Score(precision=0.015527104331244893, recall=0.7702702702702703, fmeasure=0.03044058744993324), 'rouge2': Score(precision=0.013675493080527406, recall=0.6802168021680217, fmeasure=0.02681194253057737), 'rougeL': Score(precision=0.015200217924271316, recall=0.754054054054054, fmeasure=0.02979973297730307)}	0.103003926102773
summaries_1last_t5_base-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.7287234042553191, recall=0.35584415584415585, fmeasure=0.47818499127399644), 'rouge2': Score(precision=0.20855614973262032, recall=0.1015625, fmeasure=0.13660245183887917), 'rougeL': Score(precision=0.3617021276595745, recall=0.17662337662337663, fmeasure=0.23734729493891799)}	0.34177716087726356
information_retrieval_aleph-bert-ft.csv	0.052752293577981654	0.052752293577981654	0.052752293577981654	0.052752293577981654	0.002241673120687004	{'rouge1': Score(precision=0.025559971098265896, recall=0.7905027932960894, fmeasure=0.049518810148731406), 'rouge2': Score(precision=0.02267184536175594, recall=0.7030812324929971, fmeasure=0.043927196359817997), 'rougeL': Score(precision=0.02483742774566474, recall=0.7681564245810056, fmeasure=0.048118985126859144)}	0.1901125190507303
information_retrieval_google-mbert-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.0038082984882598907, recall=0.7668393782383419, fmeasure=0.007578958149300355), 'rouge2': Score(precision=0.003473830477072719, recall=0.7012987012987013, fmeasure=0.006913415867569678), 'rougeL': Score(precision=0.0037825667417175943, recall=0.7616580310880829, fmeasure=0.007527748972615893)}	0.015175343846984747
summaries_2last_legal_hebert-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.23055809698078683, recall=0.6545454545454545, fmeasure=0.34100135317997293), 'rouge2': Score(precision=0.10622710622710622, recall=0.3020833333333333, fmeasure=0.15718157181571815), 'rougeL': Score(precision=0.1253430924062214, recall=0.35584415584415585, fmeasure=0.18538565629228684)}	0.14350165980865934
information_retrieval_legal-hebert-bm.csv	0.002197802197802198	0.002197802197802198	0.002197802197802198	0.002197802197802198	0.0003921245455787594	{'rouge1': Score(precision=0.008861911987860394, recall=0.7564766839378239, fmeasure=0.01751859851211903), 'rouge2': Score(precision=0.007860633099638836, recall=0.6727272727272727, fmeasure=0.015539689206215877), 'rougeL': Score(precision=0.00858877086494689, recall=0.7331606217616581, fmeasure=0.016978641708663308)}	0.021561654331006392
question_answering_mt5-small-ft-3e5_ft_withprep3e5_3questions.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.0013989023996556548, recall=0.03523035230352303, fmeasure=0.0026909542537776857), 'rouge2': Score(precision=0.00010761945759793371, recall=0.002717391304347826, fmeasure=0.0002070393374741201), 'rougeL': Score(precision=0.0013989023996556548, recall=0.03523035230352303, fmeasure=0.0026909542537776857)}	0.07784330361181603
summaries_1last_mbert-base-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.36363636363636365, recall=0.612987012987013, fmeasure=0.4564796905222437), 'rouge2': Score(precision=0.12962962962962962, recall=0.21875, fmeasure=0.1627906976744186), 'rougeL': Score(precision=0.17565485362095531, recall=0.2961038961038961, fmeasure=0.22050290135396516)}	0.2757430875703135
summaries_2last_t5_base-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.5471698113207547, recall=0.5272727272727272, fmeasure=0.537037037037037), 'rouge2': Score(precision=0.1891891891891892, recall=0.18229166666666666, fmeasure=0.1856763925729443), 'rougeL': Score(precision=0.2371967654986523, recall=0.22857142857142856, fmeasure=0.2328042328042328)}	0.19311651990251863
question_answering_mt5-small-ft-3e5_ft_noprep3e5_3questions.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.01330150068212824, recall=0.19696969696969696, fmeasure=0.024920127795527155), 'rouge2': Score(precision=0.001364721937905152, recall=0.02030456852791878, fmeasure=0.0025575447570332483), 'rougeL': Score(precision=0.01330150068212824, recall=0.19696969696969696, fmeasure=0.024920127795527155)}	0.10628536803467731
information_retrieval_aleph-bert-bm.csv	0.0022222222222222222	0.0022222222222222222	0.0022222222222222222	0.0022222222222222222	0.0003964960836936984	{'rouge1': Score(precision=0.025404376784015224, recall=0.712, fmeasure=0.04905833716123106), 'rouge2': Score(precision=0.015605671329336759, recall=0.4385026737967914, fmeasure=0.030138748506845538), 'rougeL': Score(precision=0.020361560418648905, recall=0.5706666666666667, fmeasure=0.039320165365181445)}	0.03922641795397718
information_retrieval_legal-hebert-ft.csv	0.013245033112582781	0.013245033112582781	0.013245033112582781	0.013245033112582781	0.0006406459527905942	{'rouge1': Score(precision=0.024271844660194174, recall=0.7552083333333334, fmeasure=0.04703211157963023), 'rouge2': Score(precision=0.019753913116263497, recall=0.6161879895561357, fmeasure=0.03828061638280616), 'rougeL': Score(precision=0.022765316370940744, recall=0.7083333333333334, fmeasure=0.04411287706779112)}	0.11574753030253733
rule_based_sentences.csv	0.5406593406593406	0.5406593406593406	0.5406593406593406	0.5406593406593406	0.8015421946218761	{'rouge1': Score(precision=0.7668393782383419, recall=0.7668393782383419, fmeasure=0.7668393782383419), 'rouge2': Score(precision=0.7662337662337663, recall=0.7662337662337663, fmeasure=0.7662337662337663), 'rougeL': Score(precision=0.7668393782383419, recall=0.7668393782383419, fmeasure=0.7668393782383419)}	0.9010989010989011
summaries_2last_alephbert-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.20076923076923076, recall=0.6779220779220779, fmeasure=0.30979228486646887), 'rouge2': Score(precision=0.10161662817551963, recall=0.34375, fmeasure=0.1568627450980392), 'rougeL': Score(precision=0.11692307692307692, recall=0.3948051948051948, fmeasure=0.18041543026706233)}	0.1476578392516672
summaries_2last_google-mbert-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.19458668617410388, recall=0.6909090909090909, fmeasure=0.30365296803652964), 'rouge2': Score(precision=0.10395314787701318, recall=0.3697916666666667, fmeasure=0.16228571428571428), 'rougeL': Score(precision=0.11704462326261887, recall=0.4155844155844156, fmeasure=0.182648401826484)}	0.16611855324813102
summaries_1last_alephbert-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.37169517884914466, recall=0.6207792207792208, fmeasure=0.4649805447470818), 'rouge2': Score(precision=0.14953271028037382, recall=0.25, fmeasure=0.1871345029239766), 'rougeL': Score(precision=0.18195956454121306, recall=0.3038961038961039, fmeasure=0.22762645914396887)}	0.21723628246723575
summaries_1last_legal_hebert-bm.csv	0.0	0.0	0	0.0	0	{'rouge1': Score(precision=0.42610364683301344, recall=0.5766233766233766, fmeasure=0.49006622516556286), 'rouge2': Score(precision=0.11923076923076924, recall=0.16145833333333334, fmeasure=0.13716814159292037), 'rougeL': Score(precision=0.18234165067178504, recall=0.24675324675324675, fmeasure=0.2097130242825607)}	0.22211201321959725
