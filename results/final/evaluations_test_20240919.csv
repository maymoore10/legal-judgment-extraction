filename	precision	recall	f1	accuracy	bleu	rouge	tlnls
information_retrieval_legal-hebert_benchmark.csv	0.014150943396226415	0.014150943396226415	0.014150943396226415	0	0.00024140598286444246	{'rouge1': Score(precision=0.024981064325188655, recall=0.9647887323943662, fmeasure=0.048701121137544434), 'rouge2': Score(precision=0.0202684695552158, recall=0.7831978319783198, fmeasure=0.0395143427493232), 'rougeL': Score(precision=0.02036637024153505, recall=0.7865655471289275, fmeasure=0.03970467596390483)}	0.07487635430180277
question_answering_mbert-bm_ft_noprep3e5_q0.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.049120923365991144, recall=0.27896341463414637, fmeasure=0.08353303663129065), 'rouge2': Score(precision=0.011543624161073825, recall=0.06559877955758962, fmeasure=0.01963246204771145), 'rougeL': Score(precision=0.049120923365991144, recall=0.27896341463414637, fmeasure=0.08353303663129065)}	0.13927797638824468
question_answering_legal-hebert-bm_ft_noprep3e5_q0.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.038491674828599415, recall=0.30465116279069765, fmeasure=0.06834782608695653), 'rouge2': Score(precision=0.009207561955137623, recall=0.07292474786656322, fmeasure=0.01635066968168377), 'rougeL': Score(precision=0.038491674828599415, recall=0.30465116279069765, fmeasure=0.06834782608695653)}	0.10348951604072772
summaries_2last_mbert-base-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
summaries_1last_google-mbert-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
summaries_1last_t5_base-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
summaries_2last_legal_hebert-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	6.23589129594293e-06
question_answering_mt5-small-bm_ft_noprep3e5_q0.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.05243942914039164, recall=0.2532051282051282, fmeasure=0.08688479516084685), 'rouge2': Score(precision=0.0069721115537848604, recall=0.033707865168539325, fmeasure=0.01155433287482806), 'rougeL': Score(precision=0.05243942914039164, recall=0.2532051282051282, fmeasure=0.08688479516084685)}	0.15982132562063722
information_retrieval_aleph-bert.csv	0.007893139040680024	0.007893139040680024	0.007893139040680024	0	0.0006702975661362737	{'rouge1': Score(precision=0.003083318281380777, recall=0.6839729119638827, fmeasure=0.006138962403920427), 'rouge2': Score(precision=0.000938735429248859, recall=0.20835686053077357, fmeasure=0.0018690500008864059), 'rougeL': Score(precision=0.001897818018077607, recall=0.42099322799097066, fmeasure=0.0037786022717199988)}	0.3739005513249008
information_retrieval_legal-hebert-bm.csv	0.003076923076923077	0.003076923076923077	0.0030769230769230765	0	0.00017142952532598913	{'rouge1': Score(precision=0.024513541754921066, recall=0.9649805447470817, fmeasure=0.04781249569659997), 'rouge2': Score(precision=0.01842777864072186, recall=0.7258064516129032, fmeasure=0.035942986986159886), 'rougeL': Score(precision=0.018102742240673275, recall=0.7126181211784325, fmeasure=0.035308536568572094)}	0.09399210466713999
question_answering_legal-hebert-ft_ft_noprep3e5_q0.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.07213885373350208, recall=0.30227272727272725, fmeasure=0.11647934608086409), 'rouge2': Score(precision=0.016455696202531647, recall=0.06899166034874905, fmeasure=0.026573222368228937), 'rougeL': Score(precision=0.07213885373350208, recall=0.30227272727272725, fmeasure=0.11647934608086409)}	0.12047731519815749
question_answering_aleph-bert-ft_ft_noprep3e5_q0.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.34827264239028943, recall=0.2685385169186465, fmeasure=0.3032520325203252), 'rouge2': Score(precision=0.07570093457943926, recall=0.058357348703170026, fmeasure=0.0659072416598861), 'rougeL': Score(precision=0.2586367880485528, recall=0.1994240460763139, fmeasure=0.22520325203252034)}	0.17774538682876215
question_answering_mt5-small-ft-3e5-3q_ft_noprep3e5_q0.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.03902923644621062, recall=0.2555762081784387, fmeasure=0.0677173110071411), 'rouge2': Score(precision=0.0075230660042583395, recall=0.04930232558139535, fmeasure=0.013054187192118228), 'rougeL': Score(precision=0.03902923644621062, recall=0.2555762081784387, fmeasure=0.0677173110071411)}	0.14525540222844843
information_retrieval_legal-hebert.csv	0.0005841121495327102	0.0005841121495327102	0.0005841121495327102	0	0.00010396257175727006	{'rouge1': Score(precision=0.11916264090177134, recall=0.6369620253164557, fmeasure=0.2007660389403128), 'rouge2': Score(precision=0.03183023872679045, recall=0.1702127659574468, fmeasure=0.05363128491620112), 'rougeL': Score(precision=0.059486596570995545, recall=0.3179746835443038, fmeasure=0.10022342802425789)}	0.1943603992656685
information_retrieval_mt5-small_benchmark.csv	0.0005830903790087463	0.0005830903790087463	0.0005830903790087463	0	0.00010378055375425241	{'rouge1': Score(precision=0.0015976027007054518, recall=0.9904137235116044, fmeasure=0.0031900596247007797), 'rouge2': Score(precision=0.0014527372091982275, recall=0.901060070671378, fmeasure=0.0029007975974570483), 'rougeL': Score(precision=0.0015211000752004531, recall=0.942986881937437, fmeasure=0.0030373007837828617)}	0.04204541267793549
summaries_1last_mbert-base-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
summaries_2last_t5_base-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
information_retrieval_aleph-bert_benchmark.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0014408244287951863, recall=0.9964682139253279, fmeasure=0.0028774882077619336), 'rouge2': Score(precision=0.0013583883765143259, recall=0.9399293286219081, fmeasure=0.002712856126108472), 'rougeL': Score(precision=0.0014408244287951863, recall=0.9964682139253279, fmeasure=0.0028774882077619336)}	0.03581374664759672
rule_based_sentences.csv	0.7049562682215743	0.7049562682215743	0.7049562682215743	0	0.9900736217884006	{'rouge1': Score(precision=0.9964682139253279, recall=0.9964682139253279, fmeasure=0.9964682139253279), 'rouge2': Score(precision=0.9964664310954063, recall=0.9964664310954063, fmeasure=0.9964664310954063), 'rougeL': Score(precision=0.9964682139253279, recall=0.9964682139253279, fmeasure=0.9964682139253279)}	0.9979591836734694
question_answering_aleph-bert-bm_ft_noprep3e5_q0.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.038733860891295296, recall=0.2987951807228916, fmeasure=0.06857774910129967), 'rouge2': Score(precision=0.00718525460793502, recall=0.05546623794212219, fmeasure=0.012722411726744721), 'rougeL': Score(precision=0.038733860891295296, recall=0.2987951807228916, fmeasure=0.06857774910129967)}	0.16382433976150915
summaries_2last_alephbert-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
summaries_2last_google-mbert-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	6.2496875156242194e-06
summaries_1last_alephbert-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
summaries_1last_legal_hebert-bm.csv	0.0	0.0	0	0	0	{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}	0.0
